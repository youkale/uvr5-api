#!/usr/bin/env python3
"""
æ£€æŸ¥å½“å‰ç¯å¢ƒçš„ GPU æ”¯æŒæƒ…å†µ
ç”¨äºéªŒè¯ processor.py æ˜¯å¦èƒ½å¯ç”¨ GPU åŠ é€Ÿ
"""

import platform
import sys

def check_environment():
    print("=" * 60)
    print("GPU æ”¯æŒæ£€æŸ¥")
    print("=" * 60)
    print()
    
    # 1. æ“ä½œç³»ç»Ÿ
    os_name = platform.system()
    print(f"1ï¸âƒ£  æ“ä½œç³»ç»Ÿ: {os_name}")
    print(f"   å¹³å°: {platform.platform()}")
    print(f"   æ¶æ„: {platform.machine()}")
    
    is_linux = os_name == 'Linux'
    if is_linux:
        print("   âœ“ Linux ç³»ç»Ÿ")
    else:
        print(f"   âš ï¸  é Linux ç³»ç»Ÿï¼ŒGPU åŠ é€Ÿä»…åœ¨ Linux ä¸Šå¯ç”¨")
    print()
    
    # 2. ONNX Runtime
    print("2ï¸âƒ£  ONNX Runtime:")
    try:
        import onnxruntime as ort
        print(f"   ç‰ˆæœ¬: {ort.__version__}")
        
        providers = ort.get_available_providers()
        print(f"   å¯ç”¨ Providers: {providers}")
        
        has_cuda = 'CUDAExecutionProvider' in providers
        has_tensorrt = 'TensorrtExecutionProvider' in providers
        
        if has_cuda:
            print("   âœ“ CUDAExecutionProvider å¯ç”¨")
        if has_tensorrt:
            print("   âœ“ TensorrtExecutionProvider å¯ç”¨")
        
        if not has_cuda and not has_tensorrt:
            print("   âš ï¸  æ²¡æœ‰ GPU providers")
            print("   æç¤º: å®‰è£… onnxruntime-gpu ä»¥å¯ç”¨ CUDA")
            
    except ImportError:
        print("   âŒ ONNX Runtime æœªå®‰è£…")
        return False
    print()
    
    # 3. PyTorch (å¯é€‰)
    print("3ï¸âƒ£  PyTorch (å¯é€‰):")
    try:
        import torch
        print(f"   ç‰ˆæœ¬: {torch.__version__}")
        print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   GPU æ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"          æ˜¾å­˜: {mem_total:.1f} GB")
    except ImportError:
        print("   âš ï¸  PyTorch æœªå®‰è£…")
    print()
    
    # 4. NVIDIA é©±åŠ¨ (Linux only)
    if is_linux:
        print("4ï¸âƒ£  NVIDIA é©±åŠ¨:")
        import subprocess
        try:
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
            if result.returncode == 0:
                print("   âœ“ nvidia-smi å¯ç”¨")
                # æå–å…³é”®ä¿¡æ¯
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'Driver Version' in line or 'CUDA Version' in line:
                        print(f"   {line.strip()}")
            else:
                print("   âŒ nvidia-smi æ— æ³•è¿è¡Œ")
        except FileNotFoundError:
            print("   âŒ nvidia-smi æœªæ‰¾åˆ°")
            print("   æç¤º: éœ€è¦å®‰è£… NVIDIA é©±åŠ¨")
        print()
    
    # 5. æœ€ç»ˆåˆ¤æ–­
    print("=" * 60)
    print("ğŸ“Š æœ€ç»ˆç»“æœ:")
    print("=" * 60)
    
    gpu_enabled = is_linux and (has_cuda or has_tensorrt)
    
    if gpu_enabled:
        print("âœ… GPU åŠ é€Ÿå°†è¢«å¯ç”¨")
        print()
        print("é¢„æœŸæ—¥å¿—è¾“å‡º:")
        print("  - âœ“ CUDA Execution Provider detected")
        print("  - Hardware acceleration: CUDA available")
        print("  - ğŸš€ Enabling GPU acceleration for audio separation")
        print("  - âœ… UVR model loaded successfully with GPU acceleration")
    else:
        print("âš ï¸  GPU åŠ é€Ÿä¸ä¼šå¯ç”¨")
        print()
        print("åŸå› :")
        if not is_linux:
            print("  - é Linux ç³»ç»Ÿ")
        if not (has_cuda or has_tensorrt):
            print("  - æ²¡æœ‰ GPU Execution Providers")
        print()
        print("é¢„æœŸæ—¥å¿—è¾“å‡º:")
        print("  - GPU providers not available, using CPU")
        print("  - Hardware acceleration: CPU only")
        print("  - Running on CPU mode")
        print("  - âœ… UVR model loaded successfully (CPU mode)")
    
    print()
    print("=" * 60)
    
    # 6. æ”¹è¿›å»ºè®®
    if not gpu_enabled:
        print()
        print("ğŸ’¡ å¯ç”¨ GPU åŠ é€Ÿçš„æ­¥éª¤:")
        print()
        if not is_linux:
            print("1. åœ¨ Linux ç³»ç»Ÿä¸Šéƒ¨ç½²")
            print("   (GPU åŠ é€Ÿç›®å‰ä»…åœ¨ Linux ä¸Šå¯ç”¨)")
        else:
            print("1. å®‰è£… NVIDIA é©±åŠ¨å’Œ CUDA:")
            print("   curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb")
            print("   sudo dpkg -i cuda-keyring_1.0-1_all.deb")
            print("   sudo apt-get update")
            print("   sudo apt-get install cuda")
            print()
            print("2. å®‰è£… GPU ç‰ˆæœ¬çš„ä¾èµ–:")
            print("   pip uninstall onnxruntime")
            print("   pip install onnxruntime-gpu")
            print()
            print("3. éªŒè¯å®‰è£…:")
            print("   python3 check_gpu_support.py")
            print()
            print("4. é‡å¯æœåŠ¡:")
            print("   ./stop_local.sh")
            print("   ./start_local.sh")
    
    return gpu_enabled

if __name__ == '__main__':
    try:
        enabled = check_environment()
        sys.exit(0 if enabled else 1)
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(2)
